{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn import functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading training and validation datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converts.csv file to pandas dataframe format => prepares for Dataloader\n",
    "class load_data(Dataset): \n",
    "    def __init__(self,csvfile): \n",
    "        self.data= pd.read_csv(csvfile)\n",
    "        #Grabs all input features, in this case it is both the subtitle/word input_ids and the _ids: [all rows, all columns from 1 to 1024]\n",
    "        self.x = self.data.iloc[:-1,1:1025].values\n",
    "        #Grabs the labels. 1 for mischief, 0 for none : [all rows, just column 1025 for the labels]\n",
    "        self.y = self.data.iloc[:-1,1025:1027].values\n",
    "\n",
    "        #we convert the values into tensor.float datatypes\n",
    "        self.x_train = torch.tensor(self.x, dtype = torch.float32)\n",
    "        self.y_train = torch.tensor(self.y, dtype = torch.float32)\n",
    "\n",
    "    def __len__(self): \n",
    "        #get length for data set so we can use it for indexing\n",
    "        return len(self.y_train)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        #grab (feature,label) pairs based on the index \n",
    "        return self.x_train[idx], self.y_train[idx]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert training and testing to csv\n",
    "featureset1 = load_data('train.csv')\n",
    "featureset2 = load_data('val.csv')\n",
    "\n",
    "#Dataloader does final prep before we pass into the model(seperate into batchesm and shuffles up the data )\n",
    "training_loader = DataLoader(featureset1,batch_size=64, shuffle=True)\n",
    "validation_loader = DataLoader(featureset2, batch_size=64, shuffle =True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(Model,self).__init__()\n",
    "    #three fully connected layers that result in a final \n",
    "    self.fc1 = nn.Linear(1024,192)\n",
    "    self.fc2 = nn.Linear(192,128)\n",
    "    self.fc3 = nn.Linear(128,64)\n",
    "    self.fc4 = nn.Linear(64,2)\n",
    "\n",
    "  \n",
    "  def forward(self, input_ids):\n",
    "    x = input_ids\n",
    "    x = torch.relu(self.fc1(x))\n",
    "    x = torch.sigmoid(self.fc2(x))\n",
    "    x = torch.relu(self.fc3(x))\n",
    "    x = torch.sigmoid(self.fc4(x))\n",
    "\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "loss_fn = nn.BCELoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model,training_loader, validation_loader, optimizer,num_epochs): \n",
    "    for epoch in range(num_epochs):\n",
    "        training_loss = 0\n",
    "        for features,labels in training_loader: \n",
    "            #outputs the prediction\n",
    "            outputs = model(features)\n",
    "            #BCE to generate loss(predictions, true label)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            #Backprop.\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            training_loss += loss.item()\n",
    "            acc = float((outputs.round() == labels).float().mean())\n",
    "\n",
    "\n",
    "        #validation loss\n",
    "        validation_loss = 0\n",
    "        for features,labels in validation_loader:\n",
    "            val_outputs = model(features)\n",
    "            val_loss = loss_fn(val_outputs, labels)\n",
    "            optimizer.zero_grad()\n",
    "            validation_loss += val_loss.item()\n",
    "        \n",
    "        training_loss /= len(training_loader)\n",
    "        validation_loss /= len(validation_loader)\n",
    "\n",
    "        print(f\"Epoch {epoch}: Training loss {training_loss}, Validation loss {validation_loss}, Acc: {acc}\")\n",
    "       \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Training loss 0.6531292498111725, Validation loss 0.6493591964244843, Acc: 0.6666666865348816\n",
      "Epoch 1: Training loss 0.6495257019996643, Validation loss 0.6390639245510101, Acc: 0.65625\n",
      "Epoch 2: Training loss 0.6393823325634003, Validation loss 0.6330879628658295, Acc: 0.6979166865348816\n",
      "Epoch 3: Training loss 0.6313560605049133, Validation loss 0.6280313432216644, Acc: 0.7604166865348816\n",
      "Epoch 4: Training loss 0.630130261182785, Validation loss 0.6251413524150848, Acc: 0.6875\n",
      "Epoch 5: Training loss 0.6243195831775665, Validation loss 0.6177098453044891, Acc: 0.6979166865348816\n",
      "Epoch 6: Training loss 0.6159536838531494, Validation loss 0.6168788075447083, Acc: 0.7916666865348816\n",
      "Epoch 7: Training loss 0.6142722368240356, Validation loss 0.6074267327785492, Acc: 0.7291666865348816\n",
      "Epoch 8: Training loss 0.6119742095470428, Validation loss 0.6053817570209503, Acc: 0.6875\n",
      "Epoch 9: Training loss 0.6002068519592285, Validation loss 0.6024059057235718, Acc: 0.8125\n",
      "Epoch 10: Training loss 0.602398544549942, Validation loss 0.6004146933555603, Acc: 0.6875\n",
      "Epoch 11: Training loss 0.5939329266548157, Validation loss 0.5904642045497894, Acc: 0.7708333134651184\n",
      "Epoch 12: Training loss 0.5964533388614655, Validation loss 0.5884698629379272, Acc: 0.6458333134651184\n",
      "Epoch 13: Training loss 0.5899782776832581, Validation loss 0.5882211327552795, Acc: 0.75\n",
      "Epoch 14: Training loss 0.5838668048381805, Validation loss 0.5859010815620422, Acc: 0.75\n",
      "Epoch 15: Training loss 0.5805583298206329, Validation loss 0.5852845907211304, Acc: 0.7708333134651184\n",
      "Epoch 16: Training loss 0.5882427394390106, Validation loss 0.5799337327480316, Acc: 0.625\n",
      "Epoch 17: Training loss 0.580367386341095, Validation loss 0.5791487395763397, Acc: 0.7291666865348816\n",
      "Epoch 18: Training loss 0.5766811370849609, Validation loss 0.5749520361423492, Acc: 0.7291666865348816\n",
      "Epoch 19: Training loss 0.5704255700111389, Validation loss 0.5699475705623627, Acc: 0.75\n",
      "Epoch 20: Training loss 0.5682777166366577, Validation loss 0.5649405121803284, Acc: 0.7291666865348816\n",
      "Epoch 21: Training loss 0.5614651143550873, Validation loss 0.5626187920570374, Acc: 0.7708333134651184\n",
      "Epoch 22: Training loss 0.5626218616962433, Validation loss 0.5649885833263397, Acc: 0.7708333134651184\n",
      "Epoch 23: Training loss 0.5620874166488647, Validation loss 0.5630906224250793, Acc: 0.7291666865348816\n",
      "Epoch 24: Training loss 0.5663217306137085, Validation loss 0.5634832382202148, Acc: 0.6875\n",
      "Epoch 25: Training loss 0.5525242835283279, Validation loss 0.5548421144485474, Acc: 0.8125\n",
      "Epoch 26: Training loss 0.5574982166290283, Validation loss 0.5545271933078766, Acc: 0.7291666865348816\n",
      "Epoch 27: Training loss 0.5579294860363007, Validation loss 0.5495168268680573, Acc: 0.7083333134651184\n",
      "Epoch 28: Training loss 0.548506110906601, Validation loss 0.5514321625232697, Acc: 0.7708333134651184\n",
      "Epoch 29: Training loss 0.5579961240291595, Validation loss 0.5464683175086975, Acc: 0.6875\n",
      "Epoch 30: Training loss 0.5464745163917542, Validation loss 0.5499890744686127, Acc: 0.7708333134651184\n",
      "Epoch 31: Training loss 0.5436614155769348, Validation loss 0.5406486988067627, Acc: 0.75\n",
      "Epoch 32: Training loss 0.5431905388832092, Validation loss 0.5361593067646027, Acc: 0.7083333134651184\n",
      "Epoch 33: Training loss 0.5394738018512726, Validation loss 0.5407930612564087, Acc: 0.7291666865348816\n",
      "Epoch 34: Training loss 0.5429012179374695, Validation loss 0.5359530448913574, Acc: 0.6875\n",
      "Epoch 35: Training loss 0.5288563817739487, Validation loss 0.5335072129964828, Acc: 0.8125\n",
      "Epoch 36: Training loss 0.53729447722435, Validation loss 0.541813850402832, Acc: 0.75\n",
      "Epoch 37: Training loss 0.5427586138248444, Validation loss 0.5381346046924591, Acc: 0.6875\n",
      "Epoch 38: Training loss 0.5344424247741699, Validation loss 0.5454039573669434, Acc: 0.7291666865348816\n",
      "Epoch 39: Training loss 0.5316109359264374, Validation loss 0.5322501957416534, Acc: 0.75\n",
      "Epoch 40: Training loss 0.5333900153636932, Validation loss 0.5413314551115036, Acc: 0.7291666865348816\n",
      "Epoch 41: Training loss 0.5322470664978027, Validation loss 0.5237193405628204, Acc: 0.7291666865348816\n",
      "Epoch 42: Training loss 0.5242897421121597, Validation loss 0.5334177315235138, Acc: 0.8125\n",
      "Epoch 43: Training loss 0.5305642485618591, Validation loss 0.5307265818119049, Acc: 0.7083333134651184\n",
      "Epoch 44: Training loss 0.5311568379402161, Validation loss 0.5244985669851303, Acc: 0.6875\n",
      "Epoch 45: Training loss 0.5239368081092834, Validation loss 0.5235220044851303, Acc: 0.7916666865348816\n",
      "Epoch 46: Training loss 0.5352046638727188, Validation loss 0.5192092061042786, Acc: 0.6875\n",
      "Epoch 47: Training loss 0.5331735163927078, Validation loss 0.5277688205242157, Acc: 0.6666666865348816\n",
      "Epoch 48: Training loss 0.5245239734649658, Validation loss 0.5174547433853149, Acc: 0.75\n",
      "Epoch 49: Training loss 0.5208461284637451, Validation loss 0.5270673036575317, Acc: 0.7708333134651184\n",
      "Epoch 50: Training loss 0.5230058431625366, Validation loss 0.5309575945138931, Acc: 0.7291666865348816\n",
      "Epoch 51: Training loss 0.5208442658185959, Validation loss 0.5218296945095062, Acc: 0.75\n",
      "Epoch 52: Training loss 0.5243183672428131, Validation loss 0.5198408961296082, Acc: 0.6875\n",
      "Epoch 53: Training loss 0.5167297869920731, Validation loss 0.5056173205375671, Acc: 0.75\n",
      "Epoch 54: Training loss 0.5206790417432785, Validation loss 0.5118807405233383, Acc: 0.6666666865348816\n",
      "Epoch 55: Training loss 0.5157590210437775, Validation loss 0.5120090246200562, Acc: 0.7291666865348816\n",
      "Epoch 56: Training loss 0.512318879365921, Validation loss 0.5097413659095764, Acc: 0.7083333134651184\n",
      "Epoch 57: Training loss 0.5111114680767059, Validation loss 0.5087269991636276, Acc: 0.7083333134651184\n",
      "Epoch 58: Training loss 0.5106856971979141, Validation loss 0.5169498175382614, Acc: 0.75\n",
      "Epoch 59: Training loss 0.5030166208744049, Validation loss 0.5010196715593338, Acc: 0.7708333134651184\n",
      "Epoch 60: Training loss 0.5034990459680557, Validation loss 0.49930647015571594, Acc: 0.75\n",
      "Epoch 61: Training loss 0.497676357626915, Validation loss 0.4919728487730026, Acc: 0.7708333134651184\n",
      "Epoch 62: Training loss 0.4939833730459213, Validation loss 0.502018079161644, Acc: 0.75\n",
      "Epoch 63: Training loss 0.49139316380023956, Validation loss 0.4910307675600052, Acc: 0.7916666865348816\n",
      "Epoch 64: Training loss 0.49042604863643646, Validation loss 0.4949033707380295, Acc: 0.7916666865348816\n",
      "Epoch 65: Training loss 0.4999411702156067, Validation loss 0.48987676203250885, Acc: 0.6875\n",
      "Epoch 66: Training loss 0.4831356704235077, Validation loss 0.4910612553358078, Acc: 0.7916666865348816\n",
      "Epoch 67: Training loss 0.4838750511407852, Validation loss 0.48704390227794647, Acc: 0.8125\n",
      "Epoch 68: Training loss 0.4850919246673584, Validation loss 0.48862655460834503, Acc: 0.7291666865348816\n",
      "Epoch 69: Training loss 0.49473971128463745, Validation loss 0.48321954905986786, Acc: 0.6458333134651184\n",
      "Epoch 70: Training loss 0.48564372956752777, Validation loss 0.4818209409713745, Acc: 0.7083333134651184\n",
      "Epoch 71: Training loss 0.48147767782211304, Validation loss 0.4794158786535263, Acc: 0.7291666865348816\n",
      "Epoch 72: Training loss 0.47846873104572296, Validation loss 0.4781399816274643, Acc: 0.75\n",
      "Epoch 73: Training loss 0.480983629822731, Validation loss 0.48388513922691345, Acc: 0.7083333134651184\n",
      "Epoch 74: Training loss 0.4838237762451172, Validation loss 0.47463497519493103, Acc: 0.6875\n",
      "Epoch 75: Training loss 0.4844164699316025, Validation loss 0.47136135399341583, Acc: 0.6875\n",
      "Epoch 76: Training loss 0.4689329266548157, Validation loss 0.4688793271780014, Acc: 0.7916666865348816\n",
      "Epoch 77: Training loss 0.46433529257774353, Validation loss 0.4723769575357437, Acc: 0.8125\n",
      "Epoch 78: Training loss 0.47066913545131683, Validation loss 0.4739033132791519, Acc: 0.7708333134651184\n",
      "Epoch 79: Training loss 0.46863000094890594, Validation loss 0.46404558420181274, Acc: 0.75\n",
      "Epoch 80: Training loss 0.46260033547878265, Validation loss 0.47127698361873627, Acc: 0.8125\n",
      "Epoch 81: Training loss 0.4688572585582733, Validation loss 0.46881771087646484, Acc: 0.6875\n",
      "Epoch 82: Training loss 0.4661187678575516, Validation loss 0.4792126417160034, Acc: 0.7708333134651184\n",
      "Epoch 83: Training loss 0.4699745327234268, Validation loss 0.4708854407072067, Acc: 0.7083333134651184\n",
      "Epoch 84: Training loss 0.47129181027412415, Validation loss 0.45746058225631714, Acc: 0.6458333134651184\n",
      "Epoch 85: Training loss 0.4582937955856323, Validation loss 0.4675793945789337, Acc: 0.7395833134651184\n",
      "Epoch 86: Training loss 0.46535998582839966, Validation loss 0.456453412771225, Acc: 0.7083333134651184\n",
      "Epoch 87: Training loss 0.46051420271396637, Validation loss 0.45461493730545044, Acc: 0.71875\n",
      "Epoch 88: Training loss 0.45795483887195587, Validation loss 0.4607798308134079, Acc: 0.6875\n",
      "Epoch 89: Training loss 0.4519485533237457, Validation loss 0.4568830579519272, Acc: 0.75\n",
      "Epoch 90: Training loss 0.44771286845207214, Validation loss 0.45498667657375336, Acc: 0.7604166865348816\n",
      "Epoch 91: Training loss 0.45663850009441376, Validation loss 0.454995334148407, Acc: 0.7083333134651184\n",
      "Epoch 92: Training loss 0.44649627804756165, Validation loss 0.4404127299785614, Acc: 0.7395833134651184\n",
      "Epoch 93: Training loss 0.4484891891479492, Validation loss 0.4445091038942337, Acc: 0.6979166865348816\n",
      "Epoch 94: Training loss 0.44610774517059326, Validation loss 0.4517664611339569, Acc: 0.7083333134651184\n",
      "Epoch 95: Training loss 0.44008752703666687, Validation loss 0.444759264588356, Acc: 0.7604166865348816\n",
      "Epoch 96: Training loss 0.4389475882053375, Validation loss 0.4428563117980957, Acc: 0.78125\n",
      "Epoch 97: Training loss 0.4348898082971573, Validation loss 0.4400244206190109, Acc: 0.78125\n",
      "Epoch 98: Training loss 0.4379073232412338, Validation loss 0.426713228225708, Acc: 0.7291666865348816\n",
      "Epoch 99: Training loss 0.4287037253379822, Validation loss 0.4291192442178726, Acc: 0.78125\n"
     ]
    }
   ],
   "source": [
    "train(model, training_loader, validation_loader, optimizer, 100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
