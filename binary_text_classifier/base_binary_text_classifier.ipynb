{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn import functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading training and validation datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converts.csv file to pandas dataframe format => prepares for Dataloader\n",
    "class load_data(Dataset): \n",
    "    def __init__(self,csvfile): \n",
    "        self.data= pd.read_csv(csvfile)\n",
    "        #Grabs all input features, in this case it is both the subtitle/word input_ids and the _ids: [all rows, all columns from 1 to 1024]\n",
    "        self.x = self.data.iloc[:-1,1:1025].values\n",
    "        #Grabs the labels. 1 for mischief, 0 for none : [all rows, just column 1025 for the labels]\n",
    "        self.y = self.data.iloc[:-1,1025:1026].values\n",
    "\n",
    "        #we convert the values into tensor.float datatypes\n",
    "        self.x_train = torch.tensor(self.x, dtype = torch.float32)\n",
    "        self.y_train = torch.tensor(self.y, dtype = torch.float32)\n",
    "\n",
    "    def __len__(self): \n",
    "        #get length for data set so we can use it for indexing\n",
    "        return len(self.y_train)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        #grab (feature,label) pairs based on the index \n",
    "        return self.x_train[idx], self.y_train[idx]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert training and testing to csv\n",
    "featureset1 = load_data('train.csv')\n",
    "featureset2 = load_data('val.csv')\n",
    "\n",
    "#Dataloader does final prep before we pass into the model(seperate into batchesm and shuffles up the data )\n",
    "training_loader = DataLoader(featureset1,batch_size=50, shuffle=True)\n",
    "validation_loader = DataLoader(featureset2, batch_size=50, shuffle =True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(Model,self).__init__()\n",
    "    #three fully connected layers that result in a final \n",
    "    self.fc1 = nn.Linear(1024,512)\n",
    "    self.fc2 = nn.Linear(512,512)\n",
    "    self.fc3 = nn.Linear(512,1)\n",
    "    \n",
    "  \n",
    "  def forward(self, input_ids):\n",
    "    x = input_ids\n",
    "    x = torch.sigmoid(self.fc1(x))\n",
    "    x = torch.sigmoid(self.fc2(x))\n",
    "    x = torch.sigmoid(self.fc3(x))\n",
    "  \n",
    "\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=.25)\n",
    "#Binary Cross Entropy\n",
    "loss_fn = nn.BCELoss()\n",
    "#loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model,training_loader, validation_loader, optimizer,num_epochs): \n",
    "    for epoch in range(num_epochs):\n",
    "        training_loss = 0\n",
    "        for features,labels in training_loader: \n",
    "            #outputs the prediction\n",
    "            outputs = model(features)\n",
    "            optimizer.zero_grad()\n",
    "            #BCE to generate loss(predictions, true label)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            training_loss += loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        #validation loss\n",
    "        validation_loss = 0\n",
    "        for features,labels in validation_loader:\n",
    "            val_outputs = model(features)\n",
    "            val_loss = loss_fn(val_outputs, labels)\n",
    "            optimizer.zero_grad()\n",
    "            validation_loss += val_loss.item()\n",
    "        \n",
    "        training_loss /= len(training_loader)\n",
    "        validation_loss /= len(validation_loader)\n",
    "\n",
    "        print(f\"Epoch {epoch}: Training loss {training_loss}, Validation loss {validation_loss}\")\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Training loss 0.5799241803941273, Validation loss 0.5873525341351827\n",
      "Epoch 1: Training loss 0.5703757305939993, Validation loss 0.5296493371327718\n",
      "Epoch 2: Training loss 0.575436756724403, Validation loss 0.516759971777598\n",
      "Epoch 3: Training loss 0.5761816530000596, Validation loss 0.5198110143343607\n",
      "Epoch 4: Training loss 0.5640226432255336, Validation loss 0.5430507858594259\n",
      "Epoch 5: Training loss 0.5764341042155311, Validation loss 0.5576398372650146\n",
      "Epoch 6: Training loss 0.5781615333897727, Validation loss 0.543934166431427\n",
      "Epoch 7: Training loss 0.5911565323670706, Validation loss 0.5629330972830454\n",
      "Epoch 8: Training loss 0.5760028759638468, Validation loss 0.5134664277235667\n",
      "Epoch 9: Training loss 0.5688708396185012, Validation loss 0.5404719114303589\n",
      "Epoch 10: Training loss 0.5773288692746844, Validation loss 0.5322492520014445\n",
      "Epoch 11: Training loss 0.5713449588843754, Validation loss 0.561183234055837\n",
      "Epoch 12: Training loss 0.5806436708995274, Validation loss 0.6498864491780599\n",
      "Epoch 13: Training loss 0.5772500790300823, Validation loss 0.5800879995028178\n",
      "Epoch 14: Training loss 0.5798299340974717, Validation loss 0.5410233934720358\n",
      "Epoch 15: Training loss 0.5778601240544092, Validation loss 0.590712308883667\n",
      "Epoch 16: Training loss 0.5615755064146859, Validation loss 0.5801611741383871\n",
      "Epoch 17: Training loss 0.5744041772115798, Validation loss 0.5391291777292887\n",
      "Epoch 18: Training loss 0.5655871686481294, Validation loss 0.532895048459371\n",
      "Epoch 19: Training loss 0.5642579666205815, Validation loss 0.5671720504760742\n",
      "Epoch 20: Training loss 0.5718183772904533, Validation loss 0.5394519368807474\n",
      "Epoch 21: Training loss 0.5728550283681779, Validation loss 0.5545325875282288\n",
      "Epoch 22: Training loss 0.5718960804598672, Validation loss 0.552317480246226\n",
      "Epoch 23: Training loss 0.5803518635886056, Validation loss 0.5745237668355306\n",
      "Epoch 24: Training loss 0.5694461989970434, Validation loss 0.5539516806602478\n",
      "Epoch 25: Training loss 0.5635129071417309, Validation loss 0.5164727767308553\n",
      "Epoch 26: Training loss 0.5694606304168701, Validation loss 0.5634080767631531\n",
      "Epoch 27: Training loss 0.5672284946555183, Validation loss 0.5962981383005778\n",
      "Epoch 28: Training loss 0.5575395808333442, Validation loss 0.5515793859958649\n",
      "Epoch 29: Training loss 0.5652141187872205, Validation loss 0.5751920938491821\n",
      "Epoch 30: Training loss 0.5694403563226972, Validation loss 0.5335295697053274\n",
      "Epoch 31: Training loss 0.5651178132920038, Validation loss 0.5121866563955942\n",
      "Epoch 32: Training loss 0.5643524158568609, Validation loss 0.5358084241549174\n",
      "Epoch 33: Training loss 0.5678427148432958, Validation loss 0.5146789252758026\n",
      "Epoch 34: Training loss 0.5690944975330716, Validation loss 0.5914223790168762\n",
      "Epoch 35: Training loss 0.5743205462183271, Validation loss 0.5405754446983337\n",
      "Epoch 36: Training loss 0.5604032746383122, Validation loss 0.5598415732383728\n",
      "Epoch 37: Training loss 0.5759190775099254, Validation loss 0.6025656859079996\n",
      "Epoch 38: Training loss 0.5690627821854183, Validation loss 0.5307798385620117\n",
      "Epoch 39: Training loss 0.562942909342902, Validation loss 0.5619848171869913\n",
      "Epoch 40: Training loss 0.577184993596304, Validation loss 0.6083228985468546\n",
      "Epoch 41: Training loss 0.5766966726098742, Validation loss 0.6322330037752787\n",
      "Epoch 42: Training loss 0.5749235437029884, Validation loss 0.601172685623169\n",
      "Epoch 43: Training loss 0.5645809272925059, Validation loss 0.5305672983328501\n",
      "Epoch 44: Training loss 0.5595618685086569, Validation loss 0.4807754357655843\n",
      "Epoch 45: Training loss 0.5625592356636411, Validation loss 0.558694044748942\n",
      "Epoch 46: Training loss 0.557196326199032, Validation loss 0.5420431693394979\n",
      "Epoch 47: Training loss 0.558350899389812, Validation loss 0.4866640269756317\n",
      "Epoch 48: Training loss 0.5614479723430815, Validation loss 0.5458446145057678\n",
      "Epoch 49: Training loss 0.5596390877451215, Validation loss 0.5538756847381592\n",
      "Epoch 50: Training loss 0.5628700738861447, Validation loss 0.5754982829093933\n",
      "Epoch 51: Training loss 0.5547069892996833, Validation loss 0.5643196503321329\n",
      "Epoch 52: Training loss 0.5550326265039898, Validation loss 0.5632582902908325\n",
      "Epoch 53: Training loss 0.555995157786778, Validation loss 0.5074353714783987\n",
      "Epoch 54: Training loss 0.5595405527523586, Validation loss 0.5439899365107218\n",
      "Epoch 55: Training loss 0.5543738348143441, Validation loss 0.5375988483428955\n",
      "Epoch 56: Training loss 0.5532755894320351, Validation loss 0.5269168814023336\n",
      "Epoch 57: Training loss 0.5577850980418069, Validation loss 0.5312621891498566\n",
      "Epoch 58: Training loss 0.5660337195509956, Validation loss 0.5446750720342001\n",
      "Epoch 59: Training loss 0.5653894373348781, Validation loss 0.5838517944018046\n",
      "Epoch 60: Training loss 0.5597333695207324, Validation loss 0.5406400263309479\n",
      "Epoch 61: Training loss 0.5543720466750008, Validation loss 0.5577938954035441\n",
      "Epoch 62: Training loss 0.5481975064391181, Validation loss 0.5340351363023123\n",
      "Epoch 63: Training loss 0.5615727532477606, Validation loss 0.5410800874233246\n",
      "Epoch 64: Training loss 0.5450686273120698, Validation loss 0.4902915457884471\n",
      "Epoch 65: Training loss 0.5440951103255862, Validation loss 0.5171582202116648\n",
      "Epoch 66: Training loss 0.5477331734838939, Validation loss 0.508511354525884\n",
      "Epoch 67: Training loss 0.556203027566274, Validation loss 0.4817810356616974\n",
      "Epoch 68: Training loss 0.5507090120088487, Validation loss 0.53911425669988\n",
      "Epoch 69: Training loss 0.5482384874707177, Validation loss 0.4689798951148987\n",
      "Epoch 70: Training loss 0.5543094305765062, Validation loss 0.5484066406885783\n",
      "Epoch 71: Training loss 0.5470251611300877, Validation loss 0.5578681627909342\n",
      "Epoch 72: Training loss 0.5643945875621977, Validation loss 0.6093642910321554\n",
      "Epoch 73: Training loss 0.5502827976431165, Validation loss 0.5422447621822357\n",
      "Epoch 74: Training loss 0.5548673839796157, Validation loss 0.5456750790278116\n",
      "Epoch 75: Training loss 0.5531831993943169, Validation loss 0.5252536833286285\n",
      "Epoch 76: Training loss 0.545085574899401, Validation loss 0.5061411956946055\n",
      "Epoch 77: Training loss 0.5482937452338991, Validation loss 0.5624863505363464\n",
      "Epoch 78: Training loss 0.5535342026324499, Validation loss 0.5305032134056091\n",
      "Epoch 79: Training loss 0.5445926473254249, Validation loss 0.5471147497495016\n",
      "Epoch 80: Training loss 0.5404270206178937, Validation loss 0.5166827340920767\n",
      "Epoch 81: Training loss 0.5503477681250799, Validation loss 0.5206076999505361\n",
      "Epoch 82: Training loss 0.551639138233094, Validation loss 0.5518021782239279\n",
      "Epoch 83: Training loss 0.5495211553005945, Validation loss 0.49814581871032715\n",
      "Epoch 84: Training loss 0.5540449477377392, Validation loss 0.565481166044871\n",
      "Epoch 85: Training loss 0.5516162457920256, Validation loss 0.5596834818522135\n",
      "Epoch 86: Training loss 0.5618766390141987, Validation loss 0.6152245203653971\n",
      "Epoch 87: Training loss 0.5606640151568821, Validation loss 0.5644887487093607\n",
      "Epoch 88: Training loss 0.5403704245885214, Validation loss 0.49645519256591797\n",
      "Epoch 89: Training loss 0.5507816076278687, Validation loss 0.5419161319732666\n",
      "Epoch 90: Training loss 0.5487659318106515, Validation loss 0.5991572340329488\n",
      "Epoch 91: Training loss 0.5639898791199639, Validation loss 0.5606723924477895\n",
      "Epoch 92: Training loss 0.5440429292974018, Validation loss 0.5720641414324442\n",
      "Epoch 93: Training loss 0.5533974042960575, Validation loss 0.5322153866291046\n",
      "Epoch 94: Training loss 0.547103552591233, Validation loss 0.5342652797698975\n",
      "Epoch 95: Training loss 0.5508634008112407, Validation loss 0.5478347738583883\n",
      "Epoch 96: Training loss 0.5421397430556161, Validation loss 0.5646202166875204\n",
      "Epoch 97: Training loss 0.5445454262551808, Validation loss 0.5720039208730062\n",
      "Epoch 98: Training loss 0.5393760218506768, Validation loss 0.5440343817075094\n",
      "Epoch 99: Training loss 0.5519890785217285, Validation loss 0.5658975839614868\n",
      "Epoch 100: Training loss 0.5464838501952943, Validation loss 0.5487139821052551\n",
      "Epoch 101: Training loss 0.5373160569440751, Validation loss 0.5354848603407542\n",
      "Epoch 102: Training loss 0.5314949396110716, Validation loss 0.5211870769659678\n",
      "Epoch 103: Training loss 0.5473703415620894, Validation loss 0.5764885942141215\n",
      "Epoch 104: Training loss 0.542086926244554, Validation loss 0.5864832401275635\n",
      "Epoch 105: Training loss 0.5562743885176522, Validation loss 0.6246614456176758\n",
      "Epoch 106: Training loss 0.5443619049730755, Validation loss 0.49866654475529987\n",
      "Epoch 107: Training loss 0.5421705487228575, Validation loss 0.5478775401910146\n",
      "Epoch 108: Training loss 0.5367988802137829, Validation loss 0.5450549324353536\n",
      "Epoch 109: Training loss 0.5359903105667659, Validation loss 0.5447039206822714\n",
      "Epoch 110: Training loss 0.5425767955325899, Validation loss 0.5550769766171774\n",
      "Epoch 111: Training loss 0.531250502382006, Validation loss 0.5990199744701385\n",
      "Epoch 112: Training loss 0.540623392377581, Validation loss 0.5774301290512085\n",
      "Epoch 113: Training loss 0.5338744294075739, Validation loss 0.534158190091451\n",
      "Epoch 114: Training loss 0.5408543163821811, Validation loss 0.5396362741788229\n",
      "Epoch 115: Training loss 0.5461088163512093, Validation loss 0.562422494093577\n",
      "Epoch 116: Training loss 0.5320953755151658, Validation loss 0.5463537573814392\n",
      "Epoch 117: Training loss 0.5328902020340874, Validation loss 0.5598185658454895\n",
      "Epoch 118: Training loss 0.5439839391481309, Validation loss 0.5365584294001261\n",
      "Epoch 119: Training loss 0.5368875761826833, Validation loss 0.5493541459242502\n",
      "Epoch 120: Training loss 0.5361646967274802, Validation loss 0.48095665375391644\n",
      "Epoch 121: Training loss 0.5455545002505893, Validation loss 0.606980582078298\n",
      "Epoch 122: Training loss 0.5376024899028596, Validation loss 0.5824008385340372\n",
      "Epoch 123: Training loss 0.5460367018268222, Validation loss 0.7739805181821188\n",
      "Epoch 124: Training loss 0.5471076951140449, Validation loss 0.5164103408654531\n",
      "Epoch 125: Training loss 0.5452979263805208, Validation loss 0.5687833229700724\n",
      "Epoch 126: Training loss 0.5416316304888044, Validation loss 0.5601296623547872\n",
      "Epoch 127: Training loss 0.5268979058379218, Validation loss 0.5573347012201945\n",
      "Epoch 128: Training loss 0.5434357949665615, Validation loss 0.5983381470044454\n",
      "Epoch 129: Training loss 0.5292769287313733, Validation loss 0.47305920720100403\n",
      "Epoch 130: Training loss 0.5315963838781629, Validation loss 0.5284154911835989\n",
      "Epoch 131: Training loss 0.5378077314013526, Validation loss 0.5437567432721456\n",
      "Epoch 132: Training loss 0.5252589228607359, Validation loss 0.5188277661800385\n",
      "Epoch 133: Training loss 0.5262772213845026, Validation loss 0.6268057922522227\n",
      "Epoch 134: Training loss 0.539379977044605, Validation loss 0.5662240286668142\n",
      "Epoch 135: Training loss 0.5555132925510406, Validation loss 0.611397941907247\n",
      "Epoch 136: Training loss 0.5357069713728768, Validation loss 0.49163129925727844\n",
      "Epoch 137: Training loss 0.5339450098219372, Validation loss 0.5411794781684875\n",
      "Epoch 138: Training loss 0.5406645153250013, Validation loss 0.5686239798863729\n",
      "Epoch 139: Training loss 0.552410961616607, Validation loss 0.6547949512799581\n",
      "Epoch 140: Training loss 0.5347821840218135, Validation loss 0.510350892941157\n",
      "Epoch 141: Training loss 0.5302998210702624, Validation loss 0.6476927498976389\n",
      "Epoch 142: Training loss 0.5423937340577444, Validation loss 0.6717093785603842\n",
      "Epoch 143: Training loss 0.5301963090896606, Validation loss 0.5386586387952169\n",
      "Epoch 144: Training loss 0.5308893975757417, Validation loss 0.5263457298278809\n",
      "Epoch 145: Training loss 0.5244032371611822, Validation loss 0.5051383475462595\n",
      "Epoch 146: Training loss 0.5344678461551666, Validation loss 0.5496641000111898\n",
      "Epoch 147: Training loss 0.521050744113468, Validation loss 0.5489903191725413\n",
      "Epoch 148: Training loss 0.5433798006602696, Validation loss 0.5682011047999064\n",
      "Epoch 149: Training loss 0.5390609261535463, Validation loss 0.5899586280186971\n",
      "Epoch 150: Training loss 0.528042102143878, Validation loss 0.5522182583808899\n",
      "Epoch 151: Training loss 0.5402707585266658, Validation loss 0.6391430298487345\n",
      "Epoch 152: Training loss 0.5236963331699371, Validation loss 0.5422669450441996\n",
      "Epoch 153: Training loss 0.5238401903992608, Validation loss 0.520649919907252\n",
      "Epoch 154: Training loss 0.5253608297734034, Validation loss 0.5066235462824503\n",
      "Epoch 155: Training loss 0.525522000732876, Validation loss 0.5707478324572245\n",
      "Epoch 156: Training loss 0.5257692351227715, Validation loss 0.5290371974309286\n",
      "Epoch 157: Training loss 0.5245361611956642, Validation loss 0.5173228085041046\n",
      "Epoch 158: Training loss 0.5290333543504987, Validation loss 0.5473183989524841\n",
      "Epoch 159: Training loss 0.5354988617556435, Validation loss 0.5435757239659628\n",
      "Epoch 160: Training loss 0.517991039014998, Validation loss 0.5431663791338602\n",
      "Epoch 161: Training loss 0.5355022109690166, Validation loss 0.5469884077707926\n",
      "Epoch 162: Training loss 0.5205576902344113, Validation loss 0.5965457359949747\n",
      "Epoch 163: Training loss 0.5377745202609471, Validation loss 0.594311535358429\n",
      "Epoch 164: Training loss 0.535271630400703, Validation loss 0.5470954775810242\n",
      "Epoch 165: Training loss 0.5405430666037968, Validation loss 0.6050849954287211\n",
      "Epoch 166: Training loss 0.526711221252169, Validation loss 0.5733091235160828\n",
      "Epoch 167: Training loss 0.5199995551790509, Validation loss 0.5533851186434428\n",
      "Epoch 168: Training loss 0.5345540444056193, Validation loss 0.5946011741956075\n",
      "Epoch 169: Training loss 0.5260902728353228, Validation loss 0.5513682862122854\n",
      "Epoch 170: Training loss 0.5191886141186669, Validation loss 0.5198502540588379\n",
      "Epoch 171: Training loss 0.531388901528858, Validation loss 0.5604864954948425\n",
      "Epoch 172: Training loss 0.5313605680352166, Validation loss 0.5444611012935638\n",
      "Epoch 173: Training loss 0.5272389054298401, Validation loss 0.5547619263331095\n",
      "Epoch 174: Training loss 0.5300792711121696, Validation loss 0.5278024772802988\n",
      "Epoch 175: Training loss 0.5289361973603567, Validation loss 0.5624484916528066\n",
      "Epoch 176: Training loss 0.5222987873213631, Validation loss 0.4988283614317576\n",
      "Epoch 177: Training loss 0.5325260886124202, Validation loss 0.5836368004480997\n",
      "Epoch 178: Training loss 0.5214184735502515, Validation loss 0.48516324162483215\n",
      "Epoch 179: Training loss 0.5211817451885769, Validation loss 0.5834872523943583\n",
      "Epoch 180: Training loss 0.5286360085010529, Validation loss 0.5156769851843516\n",
      "Epoch 181: Training loss 0.517315305414654, Validation loss 0.5639246006806692\n",
      "Epoch 182: Training loss 0.5236511003403437, Validation loss 0.5380842089653015\n",
      "Epoch 183: Training loss 0.511862746306828, Validation loss 0.5543029507001241\n",
      "Epoch 184: Training loss 0.5249948302904764, Validation loss 0.5945827960968018\n",
      "Epoch 185: Training loss 0.5125842903341565, Validation loss 0.581685205300649\n",
      "Epoch 186: Training loss 0.5354215389206296, Validation loss 0.5512042840321859\n",
      "Epoch 187: Training loss 0.5159052184649876, Validation loss 0.5270338157812754\n",
      "Epoch 188: Training loss 0.5294042570250375, Validation loss 0.5779581864674886\n",
      "Epoch 189: Training loss 0.5197097318513053, Validation loss 0.5370875696341196\n",
      "Epoch 190: Training loss 0.5104665543351855, Validation loss 0.5372042655944824\n",
      "Epoch 191: Training loss 0.5297136959575471, Validation loss 0.5788112084070841\n",
      "Epoch 192: Training loss 0.5126988674913134, Validation loss 0.541404108206431\n",
      "Epoch 193: Training loss 0.521281164316904, Validation loss 0.5998242100079855\n",
      "Epoch 194: Training loss 0.5192105486279442, Validation loss 0.5502773722012838\n",
      "Epoch 195: Training loss 0.5208895816689446, Validation loss 0.6064466039339701\n",
      "Epoch 196: Training loss 0.5176530537151155, Validation loss 0.5607216556866964\n",
      "Epoch 197: Training loss 0.5081025574888501, Validation loss 0.5008904536565145\n",
      "Epoch 198: Training loss 0.5297738867146629, Validation loss 0.6158371567726135\n",
      "Epoch 199: Training loss 0.5334764350028265, Validation loss 0.6140890518824259\n",
      "Epoch 200: Training loss 0.5165302086444128, Validation loss 0.5858869949976603\n",
      "Epoch 201: Training loss 0.5156590129647937, Validation loss 0.6246847907702128\n",
      "Epoch 202: Training loss 0.5193227387609936, Validation loss 0.5939895709355673\n",
      "Epoch 203: Training loss 0.5236722387018657, Validation loss 0.5655920505523682\n",
      "Epoch 204: Training loss 0.5132057680970147, Validation loss 0.5181552072366079\n",
      "Epoch 205: Training loss 0.5091627694311596, Validation loss 0.562408318122228\n",
      "Epoch 206: Training loss 0.520579198996226, Validation loss 0.5844824512799581\n",
      "Epoch 207: Training loss 0.5140896950449262, Validation loss 0.5091849466164907\n",
      "Epoch 208: Training loss 0.5011714994907379, Validation loss 0.49738630652427673\n",
      "Epoch 209: Training loss 0.5317079070068541, Validation loss 0.6046807368596395\n",
      "Epoch 210: Training loss 0.5119192827315557, Validation loss 0.5767815709114075\n",
      "Epoch 211: Training loss 0.5125270471686408, Validation loss 0.6364568074544271\n",
      "Epoch 212: Training loss 0.5345710246335893, Validation loss 0.6151293913523356\n",
      "Epoch 213: Training loss 0.5139932675021035, Validation loss 0.5692063172658285\n",
      "Epoch 214: Training loss 0.5109519561131796, Validation loss 0.5981085101763407\n",
      "Epoch 215: Training loss 0.5177406186149234, Validation loss 0.5638295610745748\n",
      "Epoch 216: Training loss 0.517703978788285, Validation loss 0.6181841293970743\n",
      "Epoch 217: Training loss 0.5105757486252558, Validation loss 0.5929855505625407\n",
      "Epoch 218: Training loss 0.5191267345632825, Validation loss 0.6154865821202596\n",
      "Epoch 219: Training loss 0.5129622022310892, Validation loss 0.5783507625261942\n",
      "Epoch 220: Training loss 0.49948627821036745, Validation loss 0.5209213197231293\n",
      "Epoch 221: Training loss 0.50612368895894, Validation loss 0.4996783832708995\n",
      "Epoch 222: Training loss 0.5210474601813725, Validation loss 0.5620198845863342\n",
      "Epoch 223: Training loss 0.5011884286290124, Validation loss 0.5456592837969462\n",
      "Epoch 224: Training loss 0.5094142173017774, Validation loss 0.5163528124491373\n",
      "Epoch 225: Training loss 0.5094814272153945, Validation loss 0.600444495677948\n",
      "Epoch 226: Training loss 0.5182077756949833, Validation loss 0.6001206835110983\n",
      "Epoch 227: Training loss 0.5139612541312263, Validation loss 0.5606558918952942\n",
      "Epoch 228: Training loss 0.500220714580445, Validation loss 0.5308724045753479\n",
      "Epoch 229: Training loss 0.5140246195452554, Validation loss 0.5612927873929342\n",
      "Epoch 230: Training loss 0.5131794129099164, Validation loss 0.5618803302447001\n",
      "Epoch 231: Training loss 0.519343892733256, Validation loss 0.646486500898997\n",
      "Epoch 232: Training loss 0.5133171109926133, Validation loss 0.5363827347755432\n",
      "Epoch 233: Training loss 0.49925749216760906, Validation loss 0.542604903380076\n",
      "Epoch 234: Training loss 0.5041296936216808, Validation loss 0.5797692537307739\n",
      "Epoch 235: Training loss 0.5233851245471409, Validation loss 0.6687406102816263\n",
      "Epoch 236: Training loss 0.504476147038596, Validation loss 0.5393373767534891\n",
      "Epoch 237: Training loss 0.5144812918844677, Validation loss 0.5861960450808207\n",
      "Epoch 238: Training loss 0.5099646321364811, Validation loss 0.6355524261792501\n",
      "Epoch 239: Training loss 0.4998205006122589, Validation loss 0.5662207206090292\n",
      "Epoch 240: Training loss 0.5071250526677995, Validation loss 0.5213670829931895\n",
      "Epoch 241: Training loss 0.5114171363058544, Validation loss 0.5746490160624186\n",
      "Epoch 242: Training loss 0.507880999928429, Validation loss 0.5991267561912537\n",
      "Epoch 243: Training loss 0.5044629389331454, Validation loss 0.505906343460083\n",
      "Epoch 244: Training loss 0.5052107388064975, Validation loss 0.5370953182379404\n",
      "Epoch 245: Training loss 0.4964061677455902, Validation loss 0.5640784402688345\n",
      "Epoch 246: Training loss 0.5054811423733121, Validation loss 0.5782981912295023\n",
      "Epoch 247: Training loss 0.49106453146253315, Validation loss 0.5601769387722015\n",
      "Epoch 248: Training loss 0.5207611109529223, Validation loss 0.6116749246915182\n",
      "Epoch 249: Training loss 0.5063691181795937, Validation loss 0.5859026114145914\n",
      "Epoch 250: Training loss 0.5126243957451412, Validation loss 0.5706353982289633\n",
      "Epoch 251: Training loss 0.5080597840604328, Validation loss 0.7091294725735983\n",
      "Epoch 252: Training loss 0.5037787726947239, Validation loss 0.5307590365409851\n",
      "Epoch 253: Training loss 0.49822619273549035, Validation loss 0.4838189979394277\n",
      "Epoch 254: Training loss 0.502982612167086, Validation loss 0.5995431542396545\n",
      "Epoch 255: Training loss 0.50224911740848, Validation loss 0.6013243993123373\n",
      "Epoch 256: Training loss 0.5112651600724175, Validation loss 0.555400421222051\n",
      "Epoch 257: Training loss 0.495357300554003, Validation loss 0.5263667404651642\n",
      "Epoch 258: Training loss 0.5207512690907433, Validation loss 0.727189819018046\n",
      "Epoch 259: Training loss 0.5186408318224407, Validation loss 0.5642009278138479\n",
      "Epoch 260: Training loss 0.5012542406717936, Validation loss 0.5499068299929301\n",
      "Epoch 261: Training loss 0.5004204866432008, Validation loss 0.6014945308367411\n",
      "Epoch 262: Training loss 0.49800491474923636, Validation loss 0.5798754493395487\n",
      "Epoch 263: Training loss 0.5081234517551604, Validation loss 0.6295560995737711\n",
      "Epoch 264: Training loss 0.5042818898246402, Validation loss 0.5853188633918762\n",
      "Epoch 265: Training loss 0.49226902921994525, Validation loss 0.5989098449548086\n",
      "Epoch 266: Training loss 0.503389162676675, Validation loss 0.5318171679973602\n",
      "Epoch 267: Training loss 0.5028140473933447, Validation loss 0.5695986946423849\n",
      "Epoch 268: Training loss 0.49971029304322745, Validation loss 0.5617801547050476\n",
      "Epoch 269: Training loss 0.4935685083979652, Validation loss 0.5184939603010813\n",
      "Epoch 270: Training loss 0.5176708414441064, Validation loss 0.6648563146591187\n",
      "Epoch 271: Training loss 0.5024325123855046, Validation loss 0.5479682087898254\n",
      "Epoch 272: Training loss 0.5004352189245678, Validation loss 0.5599653124809265\n",
      "Epoch 273: Training loss 0.5064021448294321, Validation loss 0.6102326909701029\n",
      "Epoch 274: Training loss 0.5098803724561419, Validation loss 0.6243984699249268\n",
      "Epoch 275: Training loss 0.5216141939163208, Validation loss 0.6824188232421875\n",
      "Epoch 276: Training loss 0.5053635452474866, Validation loss 0.5669269561767578\n",
      "Epoch 277: Training loss 0.49365000071979703, Validation loss 0.5395556886990865\n",
      "Epoch 278: Training loss 0.48456313070796786, Validation loss 0.5395771861076355\n",
      "Epoch 279: Training loss 0.5118265818981897, Validation loss 0.6108840107917786\n",
      "Epoch 280: Training loss 0.49564363842918757, Validation loss 0.5233039756615957\n",
      "Epoch 281: Training loss 0.5086106615407127, Validation loss 0.5996420979499817\n",
      "Epoch 282: Training loss 0.49299857162293936, Validation loss 0.5341881712277731\n",
      "Epoch 283: Training loss 0.5005120464733669, Validation loss 0.6095302502314249\n",
      "Epoch 284: Training loss 0.49096278775305974, Validation loss 0.6329089403152466\n",
      "Epoch 285: Training loss 0.49039566374960397, Validation loss 0.5459845264752706\n",
      "Epoch 286: Training loss 0.4931302297682989, Validation loss 0.5649182001749674\n",
      "Epoch 287: Training loss 0.5070508264359974, Validation loss 0.7988345821698507\n",
      "Epoch 288: Training loss 0.5025416811307272, Validation loss 0.5736309885978699\n",
      "Epoch 289: Training loss 0.4996266748223986, Validation loss 0.56321781873703\n",
      "Epoch 290: Training loss 0.48647917594228474, Validation loss 0.5885332624117533\n",
      "Epoch 291: Training loss 0.4925327187492734, Validation loss 0.5698036154111227\n",
      "Epoch 292: Training loss 0.4874785840511322, Validation loss 0.5816099643707275\n",
      "Epoch 293: Training loss 0.5117043526399703, Validation loss 0.778284748395284\n",
      "Epoch 294: Training loss 0.498654489006315, Validation loss 0.5683417121569315\n",
      "Epoch 295: Training loss 0.4857491978577205, Validation loss 0.558334877093633\n",
      "Epoch 296: Training loss 0.47922649340970175, Validation loss 0.4985761344432831\n",
      "Epoch 297: Training loss 0.509221803574335, Validation loss 0.6904695232709249\n",
      "Epoch 298: Training loss 0.4972217735790071, Validation loss 0.5385607182979584\n",
      "Epoch 299: Training loss 0.4810211495274589, Validation loss 0.5280954639116923\n",
      "Epoch 300: Training loss 0.49991758238701595, Validation loss 0.5497593382994334\n",
      "Epoch 301: Training loss 0.4918736716111501, Validation loss 0.5240462521711985\n",
      "Epoch 302: Training loss 0.482213237455913, Validation loss 0.5691109498341879\n",
      "Epoch 303: Training loss 0.4996215417271569, Validation loss 0.6427623629570007\n",
      "Epoch 304: Training loss 0.4974059874103183, Validation loss 0.6173789699872335\n",
      "Epoch 305: Training loss 0.4874544980980101, Validation loss 0.5863694945971171\n",
      "Epoch 306: Training loss 0.4878869297958556, Validation loss 0.5751790006955465\n",
      "Epoch 307: Training loss 0.49657894600005376, Validation loss 0.5304915904998779\n",
      "Epoch 308: Training loss 0.4744995342833655, Validation loss 0.5548895796140035\n",
      "Epoch 309: Training loss 0.4962999749751318, Validation loss 0.6526497403780619\n",
      "Epoch 310: Training loss 0.5006177056403387, Validation loss 0.6927931110064188\n",
      "Epoch 311: Training loss 0.4887041932060605, Validation loss 0.6140123804410299\n",
      "Epoch 312: Training loss 0.4829014043013255, Validation loss 0.5497776170571645\n",
      "Epoch 313: Training loss 0.4915310215382349, Validation loss 0.6160428524017334\n",
      "Epoch 314: Training loss 0.48921731682050795, Validation loss 0.691234827041626\n",
      "Epoch 315: Training loss 0.4867064257462819, Validation loss 0.6086909770965576\n",
      "Epoch 316: Training loss 0.489718143429075, Validation loss 0.626082162062327\n",
      "Epoch 317: Training loss 0.47783889940806795, Validation loss 0.551913450161616\n",
      "Epoch 318: Training loss 0.4976452007180169, Validation loss 0.523705909649531\n",
      "Epoch 319: Training loss 0.48823359466734384, Validation loss 0.6011778314908346\n",
      "Epoch 320: Training loss 0.4767967661221822, Validation loss 0.5556718011697134\n",
      "Epoch 321: Training loss 0.4915737978049687, Validation loss 0.6456182599067688\n",
      "Epoch 322: Training loss 0.4939097109295073, Validation loss 0.6129970153172811\n",
      "Epoch 323: Training loss 0.4817527277129037, Validation loss 0.5240782896677653\n",
      "Epoch 324: Training loss 0.49078947873342604, Validation loss 0.5545394817988077\n",
      "Epoch 325: Training loss 0.48518069159416927, Validation loss 0.5878550410270691\n",
      "Epoch 326: Training loss 0.47624288144565763, Validation loss 0.5469655394554138\n",
      "Epoch 327: Training loss 0.4858815301032293, Validation loss 0.6242490609486898\n",
      "Epoch 328: Training loss 0.48556104870069594, Validation loss 0.6039368907610575\n",
      "Epoch 329: Training loss 0.49294146185829524, Validation loss 0.6792589426040649\n",
      "Epoch 330: Training loss 0.48260323064667837, Validation loss 0.5610338946183523\n",
      "Epoch 331: Training loss 0.47637460771061124, Validation loss 0.5436409811178843\n",
      "Epoch 332: Training loss 0.5024306163901374, Validation loss 0.6976812481880188\n",
      "Epoch 333: Training loss 0.49758575501896085, Validation loss 0.5478127102057139\n",
      "Epoch 334: Training loss 0.483431012857528, Validation loss 0.6276932954788208\n",
      "Epoch 335: Training loss 0.47411562999089557, Validation loss 0.6078462600708008\n",
      "Epoch 336: Training loss 0.48236397050675894, Validation loss 0.652957816918691\n",
      "Epoch 337: Training loss 0.5141118112064543, Validation loss 0.7577382524808248\n",
      "Epoch 338: Training loss 0.49270558782986235, Validation loss 0.7400638461112976\n",
      "Epoch 339: Training loss 0.4849751534916106, Validation loss 0.545862873395284\n",
      "Epoch 340: Training loss 0.48730253605615526, Validation loss 0.6427106062571207\n",
      "Epoch 341: Training loss 0.4800323772998083, Validation loss 0.5128433406352997\n",
      "Epoch 342: Training loss 0.48318718870480853, Validation loss 0.5737494031588236\n",
      "Epoch 343: Training loss 0.4859864924635206, Validation loss 0.6983603239059448\n",
      "Epoch 344: Training loss 0.48102206275576637, Validation loss 0.6014503935972849\n",
      "Epoch 345: Training loss 0.4762213968095325, Validation loss 0.6464972098668417\n",
      "Epoch 346: Training loss 0.48144601924078806, Validation loss 0.6356945037841797\n",
      "Epoch 347: Training loss 0.48591174256233943, Validation loss 0.5965850750605265\n",
      "Epoch 348: Training loss 0.4718324463991892, Validation loss 0.6156502564748129\n",
      "Epoch 349: Training loss 0.47896297630809603, Validation loss 0.6238653858502706\n",
      "Epoch 350: Training loss 0.4803957981722696, Validation loss 0.64149409532547\n",
      "Epoch 351: Training loss 0.48481220432690214, Validation loss 0.6694300373395284\n",
      "Epoch 352: Training loss 0.477858400061017, Validation loss 0.5564895470937093\n",
      "Epoch 353: Training loss 0.49029657670429777, Validation loss 0.6031242410341898\n",
      "Epoch 354: Training loss 0.48574680941445486, Validation loss 0.6163187821706136\n",
      "Epoch 355: Training loss 0.48252516984939575, Validation loss 0.618829349676768\n",
      "Epoch 356: Training loss 0.47101071335020517, Validation loss 0.6120519439379374\n",
      "Epoch 357: Training loss 0.4802174553984687, Validation loss 0.5478748281796774\n",
      "Epoch 358: Training loss 0.4760653603644598, Validation loss 0.5950477619965872\n",
      "Epoch 359: Training loss 0.4893882132711865, Validation loss 0.772305448849996\n",
      "Epoch 360: Training loss 0.488006408725466, Validation loss 0.6058711310227712\n",
      "Epoch 361: Training loss 0.4823210580008371, Validation loss 0.5799557169278463\n",
      "Epoch 362: Training loss 0.4767943847747076, Validation loss 0.5926639040311178\n",
      "Epoch 363: Training loss 0.47537637324560256, Validation loss 0.5838430126508077\n",
      "Epoch 364: Training loss 0.4807413589386713, Validation loss 0.554270476102829\n",
      "Epoch 365: Training loss 0.47358601859637667, Validation loss 0.5949321786562601\n",
      "Epoch 366: Training loss 0.4710412224133809, Validation loss 0.6531171202659607\n",
      "Epoch 367: Training loss 0.46758675433340524, Validation loss 0.655441959698995\n",
      "Epoch 368: Training loss 0.4749231622332618, Validation loss 0.6254120270411173\n",
      "Epoch 369: Training loss 0.4655631028470539, Validation loss 0.6450908978780111\n",
      "Epoch 370: Training loss 0.4660037159919739, Validation loss 0.5483738978703817\n",
      "Epoch 371: Training loss 0.4783466188680558, Validation loss 0.6297481656074524\n",
      "Epoch 372: Training loss 0.46454395211878274, Validation loss 0.6125204960505167\n",
      "Epoch 373: Training loss 0.481298151470366, Validation loss 0.606714109579722\n",
      "Epoch 374: Training loss 0.4718915138925825, Validation loss 0.5587443113327026\n",
      "Epoch 375: Training loss 0.4677981791042146, Validation loss 0.5474512676397959\n",
      "Epoch 376: Training loss 0.4807330597014654, Validation loss 0.6539387504259745\n",
      "Epoch 377: Training loss 0.4732292010670617, Validation loss 0.5448599954446157\n",
      "Epoch 378: Training loss 0.46603029256775264, Validation loss 0.602881133556366\n",
      "Epoch 379: Training loss 0.47930646936098736, Validation loss 0.6119048992792765\n",
      "Epoch 380: Training loss 0.4636321422599611, Validation loss 0.6061510046323141\n",
      "Epoch 381: Training loss 0.4791559250581832, Validation loss 0.6822203596433004\n",
      "Epoch 382: Training loss 0.4744356118497394, Validation loss 0.5940715869267782\n",
      "Epoch 383: Training loss 0.461810603028252, Validation loss 0.5604579945405325\n",
      "Epoch 384: Training loss 0.47934231020155404, Validation loss 0.7037907640139262\n",
      "Epoch 385: Training loss 0.46334958218392874, Validation loss 0.5929297308127085\n",
      "Epoch 386: Training loss 0.47508503567604793, Validation loss 0.7135134935379028\n",
      "Epoch 387: Training loss 0.4694720535051255, Validation loss 0.5985930760701498\n",
      "Epoch 388: Training loss 0.4736522989613669, Validation loss 0.5758017301559448\n",
      "Epoch 389: Training loss 0.4741436243057251, Validation loss 0.6869295239448547\n",
      "Epoch 390: Training loss 0.45842939615249634, Validation loss 0.6268367071946462\n",
      "Epoch 391: Training loss 0.46536057477905635, Validation loss 0.5243052840232849\n",
      "Epoch 392: Training loss 0.4676023792652857, Validation loss 0.5999772349993387\n",
      "Epoch 393: Training loss 0.47453189605758306, Validation loss 0.6116982301076254\n",
      "Epoch 394: Training loss 0.46671216118903386, Validation loss 0.6516002813975016\n",
      "Epoch 395: Training loss 0.47153800867852713, Validation loss 0.5771682659784952\n",
      "Epoch 396: Training loss 0.46709880800474257, Validation loss 0.6119543313980103\n",
      "Epoch 397: Training loss 0.47227155168851215, Validation loss 0.7056049108505249\n",
      "Epoch 398: Training loss 0.46958383208229426, Validation loss 0.6358142495155334\n",
      "Epoch 399: Training loss 0.4580783049265544, Validation loss 0.5472088555494944\n",
      "Epoch 400: Training loss 0.4701078420593625, Validation loss 0.7475720643997192\n",
      "Epoch 401: Training loss 0.47394289005370366, Validation loss 0.6283677021662394\n",
      "Epoch 402: Training loss 0.4712668103831155, Validation loss 0.678241233030955\n",
      "Epoch 403: Training loss 0.45980955305553617, Validation loss 0.6465345621109009\n",
      "Epoch 404: Training loss 0.4685469780649458, Validation loss 0.6143129467964172\n",
      "Epoch 405: Training loss 0.45704023185230436, Validation loss 0.6956426898638407\n",
      "Epoch 406: Training loss 0.45478273857207524, Validation loss 0.5747844477494558\n",
      "Epoch 407: Training loss 0.48199503194718135, Validation loss 0.8232799371083578\n",
      "Epoch 408: Training loss 0.46445909284410025, Validation loss 0.5699035624663035\n",
      "Epoch 409: Training loss 0.46452191188221886, Validation loss 0.7070486942927042\n",
      "Epoch 410: Training loss 0.4608975251515706, Validation loss 0.6372595429420471\n",
      "Epoch 411: Training loss 0.46810983476184664, Validation loss 0.7062448660532633\n",
      "Epoch 412: Training loss 0.4619053247429076, Validation loss 0.5617371300856272\n",
      "Epoch 413: Training loss 0.4730446054821923, Validation loss 0.7168414195378622\n",
      "Epoch 414: Training loss 0.4557694707598005, Validation loss 0.671644409497579\n",
      "Epoch 415: Training loss 0.45815691351890564, Validation loss 0.6146789193153381\n",
      "Epoch 416: Training loss 0.45936293119475957, Validation loss 0.6593512495358785\n",
      "Epoch 417: Training loss 0.45909475144885836, Validation loss 0.601448784271876\n",
      "Epoch 418: Training loss 0.4781922343231383, Validation loss 0.6982301473617554\n",
      "Epoch 419: Training loss 0.46111016614096506, Validation loss 0.6348983446756998\n",
      "Epoch 420: Training loss 0.4575521917570205, Validation loss 0.6499250928560892\n",
      "Epoch 421: Training loss 0.4596949021021525, Validation loss 0.572144478559494\n",
      "Epoch 422: Training loss 0.4595304472105844, Validation loss 0.6248630483945211\n",
      "Epoch 423: Training loss 0.46120828248205636, Validation loss 0.6334615151087443\n",
      "Epoch 424: Training loss 0.45943169650577365, Validation loss 0.6334041357040405\n",
      "Epoch 425: Training loss 0.45541935733386446, Validation loss 0.6291842063268026\n",
      "Epoch 426: Training loss 0.48533515277363004, Validation loss 0.9630430340766907\n",
      "Epoch 427: Training loss 0.4763290754386357, Validation loss 0.6460779507954916\n",
      "Epoch 428: Training loss 0.4608262252239954, Validation loss 0.564510186513265\n",
      "Epoch 429: Training loss 0.4569837564513797, Validation loss 0.7884307901064554\n",
      "Epoch 430: Training loss 0.4669412275155385, Validation loss 0.5946698586146036\n",
      "Epoch 431: Training loss 0.45323529839515686, Validation loss 0.5814670026302338\n",
      "Epoch 432: Training loss 0.4662904114950271, Validation loss 0.6723117828369141\n",
      "Epoch 433: Training loss 0.4551286981219337, Validation loss 0.6467575828234354\n",
      "Epoch 434: Training loss 0.45624545500392005, Validation loss 0.5980951984723409\n",
      "Epoch 435: Training loss 0.45906341785476323, Validation loss 0.6452340682347616\n",
      "Epoch 436: Training loss 0.45519205644017174, Validation loss 0.6561876535415649\n",
      "Epoch 437: Training loss 0.45614421935308547, Validation loss 0.5814501444498698\n",
      "Epoch 438: Training loss 0.4525398214658101, Validation loss 0.6133174896240234\n",
      "Epoch 439: Training loss 0.4648526892775581, Validation loss 0.5918798446655273\n",
      "Epoch 440: Training loss 0.45485245897656396, Validation loss 0.6162072420120239\n",
      "Epoch 441: Training loss 0.46349607479004634, Validation loss 0.6483081181844076\n",
      "Epoch 442: Training loss 0.46171739555540536, Validation loss 0.6470019419987997\n",
      "Epoch 443: Training loss 0.4668594683919634, Validation loss 0.8028136889139811\n",
      "Epoch 444: Training loss 0.4549796481927236, Validation loss 0.640180786450704\n",
      "Epoch 445: Training loss 0.4500089947666441, Validation loss 0.6934958497683207\n",
      "Epoch 446: Training loss 0.4486830759616125, Validation loss 0.6303450663884481\n",
      "Epoch 447: Training loss 0.45247534201258705, Validation loss 0.5640048185984293\n",
      "Epoch 448: Training loss 0.4568502349512918, Validation loss 0.6187481085459391\n",
      "Epoch 449: Training loss 0.4478088063853128, Validation loss 0.6043869058291117\n",
      "Epoch 450: Training loss 0.46385579165958224, Validation loss 0.7542698582013448\n",
      "Epoch 451: Training loss 0.45436484331176397, Validation loss 0.5927726626396179\n",
      "Epoch 452: Training loss 0.46410794910930453, Validation loss 0.7782780925432841\n",
      "Epoch 453: Training loss 0.4591935064111437, Validation loss 0.6302191813786825\n",
      "Epoch 454: Training loss 0.4547589293548039, Validation loss 0.601245661576589\n",
      "Epoch 455: Training loss 0.44808123792920795, Validation loss 0.6524090866247813\n",
      "Epoch 456: Training loss 0.4615682860215505, Validation loss 0.689664622147878\n",
      "Epoch 457: Training loss 0.4632379895164853, Validation loss 0.820330798625946\n",
      "Epoch 458: Training loss 0.46518612191790626, Validation loss 0.6036356588204702\n",
      "Epoch 459: Training loss 0.4447251884710221, Validation loss 0.6665605703989664\n",
      "Epoch 460: Training loss 0.4645961934611911, Validation loss 0.8497097889582316\n",
      "Epoch 461: Training loss 0.4579436679681142, Validation loss 0.6214609940846761\n",
      "Epoch 462: Training loss 0.46060618616285776, Validation loss 0.6124158402283987\n",
      "Epoch 463: Training loss 0.4511196215947469, Validation loss 0.5940326154232025\n",
      "Epoch 464: Training loss 0.46686025460561115, Validation loss 0.8920230666796366\n",
      "Epoch 465: Training loss 0.45739836068380446, Validation loss 0.7017910480499268\n",
      "Epoch 466: Training loss 0.44716912508010864, Validation loss 0.5941820939381918\n",
      "Epoch 467: Training loss 0.4464429602736518, Validation loss 0.5697983801364899\n",
      "Epoch 468: Training loss 0.44720844001997084, Validation loss 0.6381888588269552\n",
      "Epoch 469: Training loss 0.4745490948359172, Validation loss 0.7593092521031698\n",
      "Epoch 470: Training loss 0.44288632699421476, Validation loss 0.611398975054423\n",
      "Epoch 471: Training loss 0.46030577591487337, Validation loss 0.6912415822347006\n",
      "Epoch 472: Training loss 0.4604033785206931, Validation loss 0.826326827208201\n",
      "Epoch 473: Training loss 0.45494172544706435, Validation loss 0.6974905133247375\n",
      "Epoch 474: Training loss 0.4462399936857678, Validation loss 0.5910209814707438\n",
      "Epoch 475: Training loss 0.44737111102967037, Validation loss 0.6835257808367411\n",
      "Epoch 476: Training loss 0.4769905535947709, Validation loss 0.9450422525405884\n",
      "Epoch 477: Training loss 0.46179100161507014, Validation loss 0.6281488140424093\n",
      "Epoch 478: Training loss 0.4344773267706235, Validation loss 0.6235737999280294\n",
      "Epoch 479: Training loss 0.4463734371321542, Validation loss 0.6569968660672506\n",
      "Epoch 480: Training loss 0.4498191291377658, Validation loss 0.9358897805213928\n",
      "Epoch 481: Training loss 0.4718264213630131, Validation loss 0.6423907279968262\n",
      "Epoch 482: Training loss 0.442230917158581, Validation loss 0.6333726048469543\n",
      "Epoch 483: Training loss 0.4503863539014544, Validation loss 0.7620542446772257\n",
      "Epoch 484: Training loss 0.4527327432518914, Validation loss 0.7005944848060608\n",
      "Epoch 485: Training loss 0.4591044173354194, Validation loss 0.8078367908795675\n",
      "Epoch 486: Training loss 0.4536731739838918, Validation loss 0.6556904911994934\n",
      "Epoch 487: Training loss 0.4512230058511098, Validation loss 0.6245214541753134\n",
      "Epoch 488: Training loss 0.43652952568871634, Validation loss 0.5992042024930319\n",
      "Epoch 489: Training loss 0.43805537479264395, Validation loss 0.6015687783559164\n",
      "Epoch 490: Training loss 0.4444956268583025, Validation loss 0.728100597858429\n",
      "Epoch 491: Training loss 0.4503081057752882, Validation loss 0.7933944861094157\n",
      "Epoch 492: Training loss 0.44939935207366943, Validation loss 0.6731112202008566\n",
      "Epoch 493: Training loss 0.4468222189517248, Validation loss 0.7011086543401083\n",
      "Epoch 494: Training loss 0.43650126173382714, Validation loss 0.6020885308583578\n",
      "Epoch 495: Training loss 0.4462962817578089, Validation loss 0.6724121371905009\n",
      "Epoch 496: Training loss 0.45159529788153513, Validation loss 0.6717742681503296\n",
      "Epoch 497: Training loss 0.4558151506242298, Validation loss 0.6916703979174296\n",
      "Epoch 498: Training loss 0.43154908149015336, Validation loss 0.6194420456886292\n",
      "Epoch 499: Training loss 0.44350546172686983, Validation loss 0.6306375563144684\n"
     ]
    }
   ],
   "source": [
    "train(model, training_loader, validation_loader, optimizer, 500)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
